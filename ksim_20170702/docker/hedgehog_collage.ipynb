{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# tSNE collage\n",
    "\n",
    "This notebook will show you how you can use a convolutional neural network (convnet) to search through a large collection of images. Specifically, it will show you how you can retrieve a set of images which are similar to a query image, returning you its `n` nearest neighbors in terms of image content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collage parameters\n",
    "\n",
    "IMAGES_PATH = \"source_images/\"\n",
    "TEST_IMAGE = IMAGES_PATH + \"hedgehog_photos/palantir_retrospective_from_brian/american-river-tubing-2013_9719594465_o.jpg\"\n",
    "\n",
    "# The number of images to put in the collage. Will be sampled at random\n",
    "# from all images in IMAGES_PATH. Try ~50â€“100 to make sure things are \n",
    "# working; ~1000 for a real collage.\n",
    "MAX_NUM_IMAGES = 50\n",
    "\n",
    "# The dimensions of the collage (measured in number of images/tiles)\n",
    "COLLAGE_WIDTH  = 10\n",
    "COLLAGE_HEIGHT = 5\n",
    "\n",
    "if (COLLAGE_WIDTH * COLLAGE_HEIGHT != MAX_NUM_IMAGES):\n",
    "    print(\"ERROR: COLLAGE_WIDTH * COLLAGE_HEIGHT must equal MAX_NUM_IMAGES (%d).\" % MAX_NUM_IMAGES)\n",
    "\n",
    "# The dimension of each tile. Aspect ratio should be 9 x 7\n",
    "TILE_WIDTH = 216\n",
    "TILE_HEIGHT = 168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "from matplotlib.pyplot import imshow\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import decode_predictions, preprocess_input\n",
    "from keras.models import Model\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "from tqdm import tqdm\n",
    "\n",
    "# get_image(path) will handle the usual pre-processing steps: load an image \n",
    "# from our file system and turn it into an input vector of the correct \n",
    "# dimensions, those expected by VGG16, namely a color image of size 224x224.\n",
    "#\n",
    "# get_image will return a handle to the image itself, and a numpy array of \n",
    "# its pixels to input the network\n",
    "def get_image(path):\n",
    "    img = image.load_img(path, target_size=model.input_shape[1:3])\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return img, x\n",
    "\n",
    "# Load a previously-trained neural network (VGG16) which comes with Keras.\n",
    "# Other pre-trained networks here: https://keras.io/applications/\n",
    "model = keras.applications.VGG16(weights='imagenet', include_top=True)\n",
    "model.summary()\n",
    "\n",
    "# Now we will remove the top classification layer from our network, leaving \n",
    "# the last fully-connected layer, \"fc2 (Dense)\" as the new output layer. The \n",
    "# way we do this is by instantiating a new model called feature_extractor \n",
    "# which takes a reference to the desired input and output layers in our VGG16 \n",
    "# model. Thus, feature_extractor's output is the layer just before the \n",
    "# classification, the last 4096-neuron fully connected layer.\n",
    "feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "\n",
    "# If we run the summary() function again, we see that the architecture of \n",
    "# feat_extractor is identical to the original model, except the last layer \n",
    "# has been removed. We also know that not just the architecture is the same, \n",
    "# but the two have the same weights as well.\n",
    "feat_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we will load all of the images in a directory. This will search \n",
    "# recursively through all the folders in IMAGE_PATH. Set MAX_NUM_IMAGES to cap it at \n",
    "# some maximum number of images to load (it will grab a random subset of \n",
    "# MAX_NUM_IMAGES is less than the number of images in your directory.\n",
    "\n",
    "IMAGES_PATH = \"source_images/\"\n",
    "\n",
    "images = [os.path.join(dp, f) for dp, dn, filenames in os.walk(IMAGES_PATH) for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]\n",
    "print(\"Found %d images.\" % len(images))\n",
    "\n",
    "if MAX_NUM_IMAGES < len(images):\n",
    "    images = [images[i] for i in sorted(random.sample(xrange(len(images)), MAX_NUM_IMAGES))]\n",
    "print(\"Keeping %d images to analyze.\" % len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Optionally, make sure things are working\n",
    "\n",
    "def test_image(path):\n",
    "    img, x = get_image(path)\n",
    "    predictions = model.predict(x)\n",
    "    imshow(img)\n",
    "    for pred in decode_predictions(predictions)[0]:\n",
    "        print(\"predicted %s with probability %0.3f\" % (pred[1], pred[2]))\n",
    "        \n",
    "    # The predict function returns an array with one element per image (in \n",
    "    # our case, there is just one). Each element contains a 4096-element \n",
    "    # array, which is the activations of the last fully-connected layer in \n",
    "    # VGG16. Let's plot the array as well.\n",
    "    feat = feat_extractor.predict(x)\n",
    "    matplotlib.pyplot.figure(figsize=(16,4))\n",
    "    matplotlib.pyplot.plot(feat[0])\n",
    "    matplotlib.pyplot.show()\n",
    "    \n",
    "#test_image(TEST_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next part will take the longest. We iterate through and extract the \n",
    "# features from all the images in our images array, placing them into an \n",
    "# array called features.\n",
    "features = []\n",
    "for image_path in tqdm(images):\n",
    "    img, x = get_image(image_path);\n",
    "    feat = feat_extractor.predict(x)[0]\n",
    "    features.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Once that is done, we will take our nx4096 matrix of features (where \n",
    "# n is the number of images), and apply principal component analysis to \n",
    "# it, and keep the first 300 principal components, creating an nx300 \n",
    "# matrix called pca_features.\n",
    "features = np.array(features)\n",
    "pca = PCA(n_components=300)\n",
    "pca.fit(features)\n",
    "pca_features = pca.transform(features)\n",
    "\n",
    "# We are now ready to do our reverse image queries! The matrix \n",
    "# pca_features contains a compact representation of our images, one \n",
    "# 300-element row for each image with high-level feature detections. We \n",
    "# should expect that two similar images, which have similar content in \n",
    "# them, should have similar arrays in pca_features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: should be cosine distance rather than Euclidean distance?\n",
    "\n",
    "# Compute the euclidean distance between the PCA features of \n",
    "# query_image_idx-th image in our dataset, and the PCA features of \n",
    "# every image in the dataset (including itself, trivially 0). It then \n",
    "# returns an array of indices to the num_results (default is 5) most \n",
    "# similar images to it (not including itself).\n",
    "def get_closest_images(query_image_idx, num_results=5):\n",
    "    distances = [ distance.euclidean(pca_features[query_image_idx], feat) for feat in pca_features ]\n",
    "    idx_closest = sorted(range(len(distances)), key=lambda k: distances[k])[1:num_results+1]\n",
    "    return idx_closest\n",
    "\n",
    "def get_concatenated_images(indexes, thumb_height):\n",
    "    thumbs = []\n",
    "    for idx in indexes:\n",
    "        img = image.load_img(images[idx])\n",
    "        img = img.resize((int(img.width * thumb_height / img.height), thumb_height))\n",
    "        thumbs.append(img)\n",
    "    concat_image = np.concatenate([np.asarray(t) for t in thumbs], axis=1)\n",
    "    return concat_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can do a query on a randomly selected image in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a query on a random image\n",
    "query_image_idx = int(len(images) * random.random())\n",
    "idx_closest = get_closest_images(query_image_idx)\n",
    "query_image = get_concatenated_images([query_image_idx], 300)\n",
    "results_image = get_concatenated_images(idx_closest, 200)\n",
    "\n",
    "# display the query image\n",
    "matplotlib.pyplot.figure(figsize = (5,5))\n",
    "imshow(query_image)\n",
    "matplotlib.pyplot.title(\"query image (%d)\" % query_image_idx)\n",
    "\n",
    "# display the resulting images\n",
    "matplotlib.pyplot.figure(figsize = (16,12))\n",
    "imshow(results_image)\n",
    "matplotlib.pyplot.title(\"result images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If we are satisfied with the quality of our image vectors, now would be a good time to save them to disk for later usage. You will need these vectors to run the [next notebook on making an image t-SNE](image-tsne.ipynb).\n",
    "\n",
    "We need to save both the image features matrix (the PCA-reduced features, not the originals), as well as the array containing the paths to each image, to make sure we can line up the images to their corresponding vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save off our image paths and PCA features\n",
    "if False:\n",
    "    pickle.dump([images, pca_features], open('interim/features_hedgehogs.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Optionally, load image paths and feature vectors from a saved file. \n",
    "if False:\n",
    "    images, pca_features = pickle.load(open('interim/features_hedgehogs.p', 'r'))\n",
    "    for i, f in zip(images, pca_features)[0:5]:\n",
    "        # We can print their contents to get an idea of what they look like:\n",
    "        print(\"image: %s, features: %0.2f,%0.2f,%0.2f,%0.2f... \"%(i, f[0], f[1], f[2], f[3]))\n",
    "    \n",
    "if len(images) > MAX_NUM_IMAGES:\n",
    "    sort_order = sorted(random.sample(xrange(len(images)), MAX_NUM_IMAGES))\n",
    "    images = [images[i] for i in sort_order]\n",
    "    pca_features = [pca_features[i] for i in sort_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually a good idea to first run the vectors through a faster dimensionality reduction technique like [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) to project your data into an intermediate lower-dimensional space before using t-SNE. This improves accuracy, and cuts down on runtime since PCA is more efficient than t-SNE. Since we have already projected our data down with PCA in the previous notebook, we can proceed straight to running the t-SNE on the feature vectors. Run the command in the following cell, taking note of the arguments:\n",
    "\n",
    "- `n_components` is the number of dimensions to project down to. In principle it can be anything, but in practice t-SNE is almost always used to project to 2 or 3 dimensions for visualization purposes.\n",
    "- `learning_rate` is the step size for iterations. You usually won't need to adjust this much, but your results may vary slightly. \n",
    "- `perplexity` refers to the number of independent clusters or zones t-SNE will attempt to fit points around. Again, it is relatively robust to large changes, and usually 20-50 works best. \n",
    "- `angle` controls the speed vs accuracy tradeoff. Lower angle means better accuracy but slower, although in practice, there is usually little improvement below a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(pca_features)\n",
    "tsne = TSNE(n_components=2, learning_rate=150, perplexity=30, angle=0.2, verbose=2).fit_transform(X)\n",
    "\n",
    "# Internally, t-SNE uses an iterative approach, making small (or sometimes \n",
    "# large) adjustments to the points. By default, t-SNE will go a maximum of \n",
    "# 1000 iterations, but in practice, it often terminates early because it \n",
    "# has found a locally optimal (good enough) embedding.\n",
    "# \n",
    "# The variable `tsne` contains an array of unnormalized 2d points, \n",
    "# corresponding to the embedding. Here we normalize the embedding so that\n",
    "# it lies entirely in the range (0,1).\n",
    "\n",
    "tx, ty = tsne[:,0], tsne[:,1]\n",
    "tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx))\n",
    "ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we will compose a new RGB image where the set of images have been \n",
    "# drawn according to the t-SNE results. Adjust `width` and `height` to set \n",
    "# the size in pixels of the full image, and set `max_dim` to the pixel size \n",
    "# (on the largest size) to scale images to.\n",
    "\n",
    "width = 4000\n",
    "height = 3000\n",
    "max_dim = 100\n",
    "\n",
    "full_image = Image.new('RGB', (width, height))\n",
    "for img, x, y in tqdm(zip(images, tx, ty)):\n",
    "    tile = Image.open(img)\n",
    "    rs = max(1, tile.width/max_dim, tile.height/max_dim)\n",
    "    tile = tile.resize((int(tile.width/rs), int(tile.height/rs)), Image.ANTIALIAS)\n",
    "    full_image.paste(tile, (int((width-max_dim)*x), int((height-max_dim)*y)))\n",
    "\n",
    "matplotlib.pyplot.figure(figsize = (16,12))\n",
    "imshow(full_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optionally, save t-SNE image\n",
    "if False:\n",
    "    full_image.save(\"output/hedgehogs_tSNE.jpg\")\n",
    "\n",
    "# Optionally, save the t-SNE points and their associated image paths \n",
    "# (for further processing in another environment).\n",
    "if False:\n",
    "    tsne_path = \"../data/example-tSNE-points-hedgehogs.json\"\n",
    "    \n",
    "    data = [{\"path\":os.path.abspath(img), \"point\":[x, y]} for img, x, y in zip(images, tx, ty)]\n",
    "    with open(tsne_path, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "    \n",
    "    print(\"saved t-SNE result to %s\" % tsne_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridify with RasterFairy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import rasterfairy\n",
    "\n",
    "# We can optionally choose a grid size of rows (nx) and columns (ny), which \n",
    "# should be equal to the number of images you have. If it is less, then you \n",
    "# can simply cut the tsne and images lists to be equal to nx * ny.\n",
    "# \n",
    "# If you omit the target=(nx, ny) argument, RasterFairy will automatically \n",
    "# choose an optimal grid size to be as square-shaped as possible. RasterFairy \n",
    "# also has options for embedding them in a grid with irregular borders as \n",
    "# well (see the GitHub page for more details).\n",
    "\n",
    "# nx * ny = 1000, the number of images\n",
    "nx = COLLAGE_WIDTH\n",
    "ny = COLLAGE_HEIGHT\n",
    "\n",
    "if (nx * ny != len(images)):\n",
    "    raise Exception(\"nx * ny should equal %d.\" % len(images))\n",
    "\n",
    "# assign to grid\n",
    "grid_assignment = rasterfairy.transformPointCloud2D(tsne, target=(nx, ny))\n",
    "\n",
    "\n",
    "# Set the tile_width and tile_height variables according to how big you want \n",
    "# the individual tile images to be. The resolution of the output image is \n",
    "# tile_width * nx x tile_height * ny. The script will automatically \n",
    "# center-crop all the tiles to match the aspect ratio of tile_width / \n",
    "# tile_height.\n",
    "\n",
    "tile_width = TILE_WIDTH\n",
    "tile_height = TILE_HEIGHT\n",
    "\n",
    "full_width = tile_width * nx\n",
    "full_height = tile_height * ny\n",
    "aspect_ratio = float(tile_width) / tile_height\n",
    "\n",
    "grid_image = Image.new('RGB', (full_width, full_height))\n",
    "\n",
    "for img, grid_pos in tqdm(zip(images, grid_assignment[0])):\n",
    "    idx_x, idx_y = grid_pos\n",
    "    x, y = tile_width * idx_x, tile_height * idx_y\n",
    "    tile = Image.open(img)\n",
    "    tile_ar = float(tile.width) / tile.height  # center-crop the tile to match aspect_ratio\n",
    "    if (tile_ar > aspect_ratio):\n",
    "        margin = 0.5 * (tile.width - aspect_ratio * tile.height)\n",
    "        tile = tile.crop((margin, 0, margin + aspect_ratio * tile.height, tile.height))\n",
    "    else:\n",
    "        margin = 0.5 * (tile.height - float(tile.width) / aspect_ratio)\n",
    "        tile = tile.crop((0, margin, tile.width, margin + float(tile.width) / aspect_ratio))\n",
    "    tile = tile.resize((tile_width, tile_height), Image.ANTIALIAS)\n",
    "    grid_image.paste(tile, (int(x), int(y)))\n",
    "\n",
    "matplotlib.pyplot.figure(figsize = (16,12))\n",
    "imshow(grid_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_image.save(\"output/hedgehogs_tSNE_grid.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shortest path\n",
    "\n",
    "Another thing you can try is to do is fine a path between two images containing `n` images. The below is a naive approach to this problem which finds the closest image to the `n` vectors which are interpolated between those of the endpoint images. A better one would be to use a variant of [Dijkstra's algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm) (implementation TBD). This implementation is not particularly good; improvement TBD (suggestions are welcome!)\n",
    "\n",
    "With the naive approach, we run another principal component analysis, this time reducing down all the way to 3 dimensions. The reason for this is when there are too many dimensions and the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) sets in, most images cluster strongly around their class, and there are few images between classes.  In a low-dimensional space, this isn't as much a problem. So we first run a new PCA, saving the columns to `pca_features2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.array(features)\n",
    "pca2 = PCA(n_components=3)\n",
    "pca2.fit(features)\n",
    "pca_features2 = pca2.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define our function `get_image_path_between` which will make `num_hops` sized stops between two images, and grab the closest image at each step, then concatenate them together and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_image_path_between(query_image_idx_1, query_image_idx_2, num_hops=4):\n",
    "    path = [query_image_idx_1, query_image_idx_2]\n",
    "    for hop in range(num_hops-1):\n",
    "        t = float(hop+1) / num_hops\n",
    "        lerp_acts = t * pca_features2[query_image_idx_1] + (1.0-t) * pca_features2[query_image_idx_2]\n",
    "        distances = [distance.euclidean(lerp_acts, feat) for feat in pca_features2]\n",
    "        idx_closest = sorted(range(len(distances)), key=lambda k: distances[k])\n",
    "        path.insert(1, [i for i in idx_closest if i not in path][0])\n",
    "    return path\n",
    "\n",
    "# pick image and number of hops\n",
    "num_hops = 10\n",
    "query_image_idx_1 = int(len(images) * random.random())\n",
    "query_image_idx_2 = int(len(images) * random.random())\n",
    "\n",
    "# get path\n",
    "path = get_image_path_between(query_image_idx_1, query_image_idx_2, num_hops)\n",
    "\n",
    "# draw image\n",
    "path_image = get_concatenated_images(path, 200)\n",
    "matplotlib.pyplot.figure(figsize = (36,12))\n",
    "imshow(path_image)\n",
    "matplotlib.pyplot.title(\"result images\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
